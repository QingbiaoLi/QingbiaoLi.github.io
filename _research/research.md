---
layout: archive
title: "Individual Project"
permalink: /research/
author_profile: true
collection: portfolio
---

<!-- <hr color="000000"/> -->

{% include base_path %}


## <b>Estimation of Tissue Oxygen Saturation based on Image to Image Translation</b>
* 2018, _The Hamlyn Centre, Imperial College London, London, UK_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Motivation</b>: Investigate a non-invasive intra-operative measurement of tissue oxygen saturation based on Hyperspectral Imaging.</li>
    <li><b>Supervisor</b>: Prof.Daniel S Elson </li>
    <li><b>Abstract</b>: 
      <ul>
        <li>The conditional Generative Adversarial Networks (cGAN) was used to develop pixel-level image-to-image translation approach, called RGB2StO2, to estimate tissue oxygen saturation (StO2) from RGB images directly. </li>
        <li>Dual-input network, called Dual2StO2, was developed to investigate the optimal setting of the fibre bundle to capture meaningful and informative images from HSI camera. </li>
      </ul>
    </li>
    <li>[<u><a href="http://qingbiaoli.github.io/files/HSMR2018.pdf">RGB2StO2.pdf</a></u>],[<u><a href="https://link.springer.com/content/pdf/10.1007%2Fs11548-019-01940-2.pdf">Dual2StO2.pdf</a></u>]</li>
  </ul>
  <br/>
  <img src='/images/customized/Dual2StO2.png' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />



## <b>Academic Research Internship in Legged Robot</b>
* Summer 2017, _Intelligence Robot Lab, Zhejiang University_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. Qiuguo Zhu </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Robust control of bipedal walking for legged robot. </li>
        <li>Carried out physical experiment. </li>
      </ul>
    </li>
  </ul>
  <br/>
    <img src='/images/customized/ZJU2.gif' />
  </fieldset>
</div>
<hr color="#FFFFFF" />

## <b>Research Assistant in Bipedal Walking of Humanoid Robot</b>
* 2017, _SLMC, School of Informatics, The University of Edinburgh_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. Zhibin Li </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Research model-free control of bipedal walking for humanoid robotics. </li>
        <li>Theoretical proof and simulation validation of online parameter estimation to obtain robust control of bipedal walking.</li>
      </ul>
    </li>
    <li>[<u><a href="http://qingbiaoli.github.io/files/humanoid2017.pdf">Humanoid2017.pdf</a></u>]</li>
  </ul>
  <br/>
    <img src='/images/customized/Humanoid2017_demo.png' width="560" height="315"/>
    <img src='/images/customized/Humanoid_case2.gif' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Missile Impact on Snow (MEng Thesis with Distinction)</b>
* 2015, _The University of Edinburgh_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr Filipe Teixeira-Dias </li>
    <li><b>Project description</b>: 
      <ul>
        <li>This study aimed to optimize the design of the impactor developed by British Antarctic survey for long-term tracking on the motion of the glaciers. </li>
        <li>Investigated the characteristics of the impact dynamics of the impactor and its interaction with different types of snow, covering a range of impact energies.  </li>
      </ul>
    </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>CAD modeeling of the impactor. </li>
        <li>Signal processing of data from accelerometer, and analysed on the results.   </li>
      </ul>
    </li>
  </ul>
  <br/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Research Assistant in Industrial Robotics (Funded by Erasmus+)</b>
* 2015, _The Institute of Production Engineering and Machine Tools (IFW), Leibniz University of Hanover_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dipl.-Ing.Thomas Lepper </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Mechanism design for industrial robot for industrial-level milling process, includes CAD modelling transmission device and robot arm. </li>
        <li>Kinematic simulation to analyse torque distribution during operation. </li>
      </ul>
    </li>
  </ul>
  <br/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


# <i>Collaborative Work</i>
<!-- --- -->
## <b>Real-time Surgical Environment Enhancement for Robot-Assisted Minimally Invasive Surgery</b>
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Project description</b>: 
      <ul>
        <li>We propose a  multi-scale Generative Adversarial Network (GAN)-based video super-resolution method to construct a framework for automatic zooming ratio adjustment. </li>
        <li>It can provide automatic real-time zooming for high-quality visualization of the Region Of Interest (ROI) during the surgical operation. </li>
        <li>The framework is validated with the JIGSAW dataset and Hamlyn Centre Laparoscopic/Endoscopic Video Datasets, with results demonstrating its practicability. </li>
      </ul>
    </li>
    <li>[<u><a href="http://qingbiaoli.github.io/files/humanoid2017.pdf">ICRA2021.pdf</a></u>]</li>
  </ul>
  <br/>
 <!--  <img src='/images/customized/ORB_SLAM2_demo.gif' width="560" height="315"/>
  <img src='/images/customized/surface_reconsuction.gif' width="560" height="315"/> -->
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Vision-based Navigation in Flexible Endoscopy</b>
* 2017, _The Hamlyn Centre, Imperial College London_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. George Mylonas </li>
    <li><b>Project description</b>: 
      <ul>
        <li>Simultaneously mapping the human colon and tracking the endoscope pose in real time during flexible endoscopy. </li>
      </ul>
    </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Investigated available visual SLAM methods (ORB-SLAM) and visual-inertial SLAM methods (VINS-Mono, OKVIS), and customize them for small scale, near focus. </li>
        <li>Our SLAM pipeline can obtain conclusive registration and surface reconstruction based on point cloud data. </li>
      </ul>
    </li>
  </ul>
  <br/>
  <img src='/images/customized/ORB_SLAM2_demo.gif' width="560" height="315"/>
  <img src='/images/customized/surface_reconsuction.gif' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />



